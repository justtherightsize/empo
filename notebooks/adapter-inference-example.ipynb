{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_lm_name = 'roberta-base'\n",
    "adapter_path = 'data/trained_adapters/wassa_essay_empathy/final_adapter'\n",
    "with_adapter_head = True # True to use the prediction head\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_lm_name)\n",
    "model = AutoAdapterModel.from_pretrained(base_lm_name)\n",
    "adapter_name = model.load_adapter(adapter_path, with_head=with_adapter_head, model_name=base_lm_name)\n",
    "model.set_active_adapters(adapter_name)\n",
    "\n",
    "\n",
    "# inference example\n",
    "output = model(tokenizer(\"I wonder why there aren't more people trying to help these people. I understand Haiti is not the richest nor less corrupt country but surely there must be a way to help. Supplies being looted by crowds is understandable because they are hungry and people need food and water to survive. We must think of other ways to distribute the food and water.\", return_tensors=\"pt\").input_ids)\n",
    "\n",
    "print(f\"Empathy score predicted by adapter model: {float(output.logits[0][0].detach())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
